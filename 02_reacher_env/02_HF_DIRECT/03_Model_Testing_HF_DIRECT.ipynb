{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RizanSM/zero_shot_llms_in_HIL_RL/blob/main/02_reacher_env/02_HF_DIRECT/03_Model_Testing_HF_DIRECT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OuwmGGRJrPVF"
      },
      "outputs": [],
      "source": [
        "# Install the required libraries in your Google Colab environment\n",
        "!pip install gymnasium[mujoco] mujoco stable-baselines3 -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZhaXFBUrfBS"
      },
      "outputs": [],
      "source": [
        "# Import the necessary libraries\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import os\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oilpd3dOz8r_"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3 import PPO\n",
        "from google.colab import drive\n",
        "from google.colab import data_table\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Average Episodic Reward (AER)"
      ],
      "metadata": {
        "id": "pzCVsjo0wuUQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def average_episodic_reward(test_trajectory_df):\n",
        "    # Step 1: Calculate Cumulative Reward Per Episode\n",
        "    cumulative_rewards = test_trajectory_df.groupby('Episode')['Reward'].sum().reset_index()\n",
        "    cumulative_rewards.columns = ['episode', 'cumulative_reward']\n",
        "\n",
        "    # Step 2: Calculate Variance and Standard Deviation of Cumulative Rewards\n",
        "    variance = cumulative_rewards['cumulative_reward'].var()\n",
        "    std_dev = cumulative_rewards['cumulative_reward'].std()\n",
        "    final_reward = cumulative_rewards['cumulative_reward'].mean()\n",
        "\n",
        "    # Step 4: Return the dataframe containing all results\n",
        "    return cumulative_rewards, variance, std_dev, final_reward"
      ],
      "metadata": {
        "id": "Xcvr9RJxw8OO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "COMBINING ALL THE DATA FRAMES"
      ],
      "metadata": {
        "id": "MQHLZLMKGHxn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_cummulative_reward(df1, df2, df3, df4, df5):\n",
        "    \"\"\"\n",
        "    Create a data frame 'ideal_cummulative_reward' that contains:\n",
        "      - 'episode' column (common across all data frames)\n",
        "      - 'cumulative_reward_1' to 'cumulative_reward_5' columns from each data frame respectively,\n",
        "        where each input data frame has columns 'episode' and 'cummulative_reward'.\n",
        "      - 'mean_cumulative_reward' column containing the row-wise mean of the 5 cumulative rewards.\n",
        "\n",
        "    Parameters:\n",
        "      df1, df2, df3, df4, df5 (pd.DataFrame): Data frames with columns 'episode' and 'cummulative_reward'.\n",
        "\n",
        "    Returns:\n",
        "      pd.DataFrame: The merged and aggregated data frame.\n",
        "    \"\"\"\n",
        "    # Rename the 'cummulative_reward' column in each data frame to a unique name.\n",
        "    df1_renamed = df1.rename(columns={'cumulative_reward': 'cumulative_reward_1'})\n",
        "    df2_renamed = df2.rename(columns={'cumulative_reward': 'cumulative_reward_2'})\n",
        "    df3_renamed = df3.rename(columns={'cumulative_reward': 'cumulative_reward_3'})\n",
        "    df4_renamed = df4.rename(columns={'cumulative_reward': 'cumulative_reward_4'})\n",
        "    df5_renamed = df5.rename(columns={'cumulative_reward': 'cumulative_reward_5'})\n",
        "\n",
        "    # Merge the data frames on the 'episode' column.\n",
        "    merged_df = df1_renamed[['episode', 'cumulative_reward_1']].copy()\n",
        "    merged_df = merged_df.merge(df2_renamed[['episode', 'cumulative_reward_2']], on='episode')\n",
        "    merged_df = merged_df.merge(df3_renamed[['episode', 'cumulative_reward_3']], on='episode')\n",
        "    merged_df = merged_df.merge(df4_renamed[['episode', 'cumulative_reward_4']], on='episode')\n",
        "    merged_df = merged_df.merge(df5_renamed[['episode', 'cumulative_reward_5']], on='episode')\n",
        "\n",
        "    # Compute the episode-wise mean of the cumulative rewards.\n",
        "    reward_columns = [\n",
        "        'cumulative_reward_1',\n",
        "        'cumulative_reward_2',\n",
        "        'cumulative_reward_3',\n",
        "        'cumulative_reward_4',\n",
        "        'cumulative_reward_5'\n",
        "    ]\n",
        "    merged_df['mean_cumulative_reward'] = merged_df[reward_columns].mean(axis=1)\n",
        "\n",
        "    return merged_df"
      ],
      "metadata": {
        "id": "S-oUVinlpBFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "VARIANCE, STANDARD DEVIATION & FINAL AER (AVERAGE EPISODIC REWARD)"
      ],
      "metadata": {
        "id": "DaD1AmkUHNJP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_performance(variance_values, std_dev_values, final_reward_values):\n",
        "    \"\"\"\n",
        "    Calculate performance metrics by computing the mean of each provided list.\n",
        "\n",
        "    Parameters:\n",
        "        variance_values (list of float): A list containing 5 variance values.\n",
        "        std_dev_values (list of float): A list containing 5 standard deviation values.\n",
        "        final_reward_values (list of float): A list containing 5 final reward values.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing three values:\n",
        "            - mean_variance (float): Mean of the variance values.\n",
        "            - mean_std_dev (float): Mean of the standard deviation values.\n",
        "            - mean_final_reward (float): Mean of the final reward values.\n",
        "    \"\"\"\n",
        "    mean_variance = sum(variance_values) / len(variance_values)\n",
        "    mean_std_dev = sum(std_dev_values) / len(std_dev_values)\n",
        "    mean_final_reward = sum(final_reward_values) / len(final_reward_values)\n",
        "\n",
        "\n",
        "    return mean_variance, mean_std_dev, mean_final_reward\n"
      ],
      "metadata": {
        "id": "J2_6XYPZsGlr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "0. MODEL TESTING (HF_D_Ideal)"
      ],
      "metadata": {
        "id": "Ed-sCvL0SeNo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trajectory_df_1 = pd.read_pickle('/content/drive/MyDrive/data3_rp1/3_test_trajectories/1_hf_d_ideal/1_hf_d_reacher_ideal_df_8.pkl')     # Update directory location 1\n",
        "trajectory_df_2 = pd.read_pickle('/content/drive/MyDrive/data3_rp1/3_test_trajectories/1_hf_d_ideal/2_hf_d_reacher_ideal_df_8.pkl')     # Update directory location 2\n",
        "trajectory_df_3 = pd.read_pickle('/content/drive/MyDrive/data3_rp1/3_test_trajectories/1_hf_d_ideal/3_hf_d_reacher_ideal_df_8.pkl')     # Update directory location 3\n",
        "trajectory_df_4 = pd.read_pickle('/content/drive/MyDrive/data3_rp1/3_test_trajectories/1_hf_d_ideal/4_hf_d_reacher_ideal_df_8.pkl')     # Update directory location 4\n",
        "trajectory_df_5 = pd.read_pickle('/content/drive/MyDrive/data3_rp1/3_test_trajectories/1_hf_d_ideal/5_hf_d_reacher_ideal_df_8.pkl')     # Update directory location 5"
      ],
      "metadata": {
        "id": "OPgGDJMDSTya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TESTING THE MODEL BY ANALYZING THE DATA FRAME <br>\n",
        "\n"
      ],
      "metadata": {
        "id": "PxYsCZSWSslL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Human feedback Direct Reacher Ideal data frame 1<br>\n",
        "trajectory_df_1 ---> 1_hf_d_reacher_ideal_df"
      ],
      "metadata": {
        "id": "BYagvrJqS5UK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_table.enable_dataframe_formatter()\n",
        "data_table.DataTable(trajectory_df_1)"
      ],
      "metadata": {
        "id": "GBjRTeFLTbLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A. Average Episodic Reward (trajectory_df_1)"
      ],
      "metadata": {
        "id": "4a8IM409VhQN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_curve_result_df_1, variance_1, std_dev_1, average_episodic_reward_1 = average_episodic_reward(trajectory_df_1)\n",
        "print(\"Variance:\", variance_1)\n",
        "print(\"Standard Deviation:\", std_dev_1)\n",
        "print(\"Average Episodic Reward:\", average_episodic_reward_1)"
      ],
      "metadata": {
        "id": "7GH0WKuRUdpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_table.enable_dataframe_formatter()\n",
        "data_table.DataTable(learning_curve_result_df_1)"
      ],
      "metadata": {
        "id": "c75ysAYcUdli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Human feedback Direct Reacher Ideal data frame 2<br>\n",
        "trajectory_df_2 ---> 2_hf_d_reacher_ideal_df"
      ],
      "metadata": {
        "id": "_UHpqQeZoDxJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_table.enable_dataframe_formatter()\n",
        "data_table.DataTable(trajectory_df_2)"
      ],
      "metadata": {
        "id": "UyPVaytep57o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A. Average Episodic Reward (trajectory_df_2)"
      ],
      "metadata": {
        "id": "K7QqTFvkpqja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_curve_result_df_2, variance_2, std_dev_2, average_episodic_reward_2 = average_episodic_reward(trajectory_df_2)\n",
        "print(\"Variance:\", variance_2)\n",
        "print(\"Standard Deviation:\", std_dev_2)\n",
        "print(\"Average Episodic Reward:\", average_episodic_reward_2)"
      ],
      "metadata": {
        "id": "S06U6FPGp_Gd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_table.enable_dataframe_formatter()\n",
        "data_table.DataTable(learning_curve_result_df_2)"
      ],
      "metadata": {
        "id": "I946Xw05p_BV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Human feedback Direct Reacher Ideal data frame 3<br>\n",
        "trajectory_df_3 ---> 3_hf_d_reacher_ideal_df"
      ],
      "metadata": {
        "id": "C1o5c-DcoFPI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_table.enable_dataframe_formatter()\n",
        "data_table.DataTable(trajectory_df_3)"
      ],
      "metadata": {
        "id": "0NRngZv_6zLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A. Average Episodic Reward (trajectory_df_3)"
      ],
      "metadata": {
        "id": "3TGK9EJAprRy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_curve_result_df_3, variance_3, std_dev_3, average_episodic_reward_3 = average_episodic_reward(trajectory_df_3)\n",
        "print(\"Variance:\", variance_3)\n",
        "print(\"Standard Deviation:\", std_dev_3)\n",
        "print(\"Average Episodic Reward:\", average_episodic_reward_3)"
      ],
      "metadata": {
        "id": "rd3COSdDo92z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_table.enable_dataframe_formatter()\n",
        "data_table.DataTable(learning_curve_result_df_3)"
      ],
      "metadata": {
        "id": "mNcWrYzmpd2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Human feedback Direct Reacher Ideal data frame 4<br>\n",
        "trajectory_df_4 ---> 4_hf_d_reacher_ideal_df"
      ],
      "metadata": {
        "id": "tHLNVDRyoFYq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_table.enable_dataframe_formatter()\n",
        "data_table.DataTable(trajectory_df_4)"
      ],
      "metadata": {
        "id": "j_3vaijG8sKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A. Average Episodic Reward (trajectory_df_4)"
      ],
      "metadata": {
        "id": "dlUweOkJpr99"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_curve_result_df_4, variance_4, std_dev_4, average_episodic_reward_4 = average_episodic_reward(trajectory_df_4)\n",
        "print(\"Variance:\", variance_4)\n",
        "print(\"Standard Deviation:\", std_dev_4)\n",
        "print(\"Average Episodic Reward:\", average_episodic_reward_4)"
      ],
      "metadata": {
        "id": "ip1kBdvLo2ko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_table.enable_dataframe_formatter()\n",
        "data_table.DataTable(learning_curve_result_df_4)"
      ],
      "metadata": {
        "id": "M_JVqq8upqMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Human feedback Direct Reacher Ideal data frame 5<br>\n",
        "trajectory_df_5 ---> 5_hf_d_reacher_ideal_df"
      ],
      "metadata": {
        "id": "K3TzZlChoFnb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_table.enable_dataframe_formatter()\n",
        "data_table.DataTable(trajectory_df_5)"
      ],
      "metadata": {
        "id": "isGcGzXV8vtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A. Average Episodic Reward (trajectory_df_5)"
      ],
      "metadata": {
        "id": "Qf7Fe3K1ps1u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_curve_result_df_5, variance_5, std_dev_5, average_episodic_reward_5 = average_episodic_reward(trajectory_df_5)\n",
        "print(\"Variance:\", variance_5)\n",
        "print(\"Standard Deviation:\", std_dev_5)\n",
        "print(\"Average Episodic Reward:\", average_episodic_reward_5)"
      ],
      "metadata": {
        "id": "yJ7ovpDaokPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_table.enable_dataframe_formatter()\n",
        "data_table.DataTable(learning_curve_result_df_5)"
      ],
      "metadata": {
        "id": "QQRHEWSIpz5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "COMBINED DATA FRAMES - HF-D-IDEAL"
      ],
      "metadata": {
        "id": "kADsiL9jwfYv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cummulative_reward = create_cummulative_reward(\n",
        "        learning_curve_result_df_1,\n",
        "        learning_curve_result_df_2,\n",
        "        learning_curve_result_df_3,\n",
        "        learning_curve_result_df_4,\n",
        "        learning_curve_result_df_5\n",
        "    )\n",
        "\n",
        "data_table.enable_dataframe_formatter()\n",
        "data_table.DataTable(cummulative_reward)"
      ],
      "metadata": {
        "id": "Fz-dCTT6tCS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "VARIANCE, STANDARD DEVIATION & FINAL REWARD : HF-D-IDEAL"
      ],
      "metadata": {
        "id": "Yl9DkFyLwpEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # Organize the values into lists\n",
        "variance_list = [variance_1, variance_2, variance_3, variance_4, variance_5]\n",
        "std_dev_list = [std_dev_1, std_dev_2, std_dev_3, std_dev_4, std_dev_5]\n",
        "final_reward_list = [average_episodic_reward_1, average_episodic_reward_2, average_episodic_reward_3, average_episodic_reward_4, average_episodic_reward_5]\n",
        "# Call the function to compute the mean performance values\n",
        "mean_variance, mean_std_dev, mean_final_reward = calculate_performance(variance_list, std_dev_list, final_reward_list)\n",
        "\n",
        "# Print the results\n",
        "print(\"HF_D_IDEAL Mean Variance:\", mean_variance)\n",
        "print(\"HF_D_IDEAL  Mean Standard Deviation:\", mean_std_dev)\n",
        "print(\"HF_D_IDEAL  Mean Average Episodic Reward:\", mean_final_reward)\n"
      ],
      "metadata": {
        "id": "UcSS6Ikpwlkw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOOe6prv/S4/tkkf6OMkgD7",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}