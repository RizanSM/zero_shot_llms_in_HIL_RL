{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNLpVAagMLTIqFy82nBHP+E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RizanSM/zero_shot_llms_in_HIL_RL/blob/main/02_reacher_env/01_generating_trajectories/01_generating_trajectories_reacher.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rkfIqqUIRyPR"
      },
      "outputs": [],
      "source": [
        "!pip install gymnasium[mujoco] mujoco stable-baselines3 -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary libraries\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import os\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "TFlE1QtdS6Rp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from google.colab import data_table\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "dOrjROH9TEfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to save the training logs\n",
        "log_dir = \"/content/drive/MyDrive/data3_rp1/0_log_dir/0_ppo_reacher_initial\"         # Update directory location 1\n",
        "os.makedirs(log_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "hJBDf1V1TN7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the environment\n",
        "env = gym.make(\"Reacher-v5\")\n",
        "env = Monitor(env, log_dir)"
      ],
      "metadata": {
        "id": "jbZ3HS4FTKEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1.2: Initial Observation\n",
        "# Print out a sample observation to see what the agent receives at the start.\n",
        "obs = env.reset()\n",
        "print(\"Initial Observation: \", obs)"
      ],
      "metadata": {
        "id": "UEt2U_bLTYEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1.3: Implement PPO with the Highway environment\n",
        "# Initialize the PPO algorithm with the chosen environment.\n",
        "# Create the PPO model with the Highway environment\n",
        "model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=log_dir)"
      ],
      "metadata": {
        "id": "6F5Zk7gMTdP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1.4: Train the model\n",
        "# 10,000 timesteps for initial training\n",
        "model.learn(total_timesteps=10000)"
      ],
      "metadata": {
        "id": "oaYeY3ViUS_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_path = os.path.join(log_dir, \"monitor.csv\")\n",
        "df = pd.read_csv(log_path, skiprows=1)\n",
        "df.rename(columns={\"index\": \"episode\", \"r\": \"reward\", \"l\": \"length\", \"t\": \"time_step\"}, inplace=True)"
      ],
      "metadata": {
        "id": "XpGGaFmzUTwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1.5: Save the trained model to Google Drive\n",
        "model.save('/content/drive/MyDrive/data3_rp1/1_trained_models/0_ppo_reacher_intial_training')         # Update directory location 2"
      ],
      "metadata": {
        "id": "1i5DcdSzUYzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1.6: Load the saved PPO model from Google Drive\n",
        "model = PPO.load('/content/drive/MyDrive/data3_rp1/1_trained_models/0_ppo_reacher_intial_training')   # Update directory location 3"
      ],
      "metadata": {
        "id": "4Jl_tJLwUjMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TRAJECTORY GENERATION AND COLLECTION"
      ],
      "metadata": {
        "id": "SZVT8n86UrsQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_reacher_trajectories(env, model, num_episodes):\n",
        "    \"\"\"\n",
        "    Generates trajectories for the Reacher-v5 environment.\n",
        "\n",
        "    Parameters:\n",
        "        env (gym.Env): The wrapped Gymnasium environment.\n",
        "        model (stable_baselines3.PPO): The trained PPO model.\n",
        "        num_episodes (int): Number of episodes to run.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame containing trajectory data.\n",
        "    \"\"\"\n",
        "    trajectory_data = []\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state, _ = env.reset()\n",
        "        timestep = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            # Get action from the trained PPO model\n",
        "            action, _ = model.predict(state)\n",
        "\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "            # Store the transition\n",
        "            trajectory_data.append({\n",
        "                \"Episode\": episode + 1,\n",
        "                \"Timestep\": timestep,\n",
        "                \"State\": state,\n",
        "                \"Action\": action,\n",
        "                \"Reward\": reward,\n",
        "                \"Next State\": next_state\n",
        "            })\n",
        "\n",
        "            # Update state and timestep\n",
        "            state = next_state\n",
        "            timestep += 1\n",
        "            done = terminated or truncated\n",
        "\n",
        "    return pd.DataFrame(trajectory_data)"
      ],
      "metadata": {
        "id": "wsohL1hKUsl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trajectory_df = generate_reacher_trajectories(env, model, num_episodes=100)"
      ],
      "metadata": {
        "id": "jlw_VkAyW_xF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_table.enable_dataframe_formatter()\n",
        "data_table.DataTable(trajectory_df)"
      ],
      "metadata": {
        "id": "fdfzgvxHXFu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the data type of each column\n",
        "print(type(trajectory_df['Episode'][0]))\n",
        "print(type(trajectory_df['Timestep'][0]))\n",
        "print(type(trajectory_df['State'][0]))\n",
        "print(type(trajectory_df['Action'][0]))\n",
        "print(type(trajectory_df['Reward'][0]))\n",
        "print(type(trajectory_df['Next State'][0]))"
      ],
      "metadata": {
        "id": "5bI0Fh3dXI_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to save the DataFrame (adjust the path as necessary)\n",
        "trajectory_df_path = '/content/drive/MyDrive/data3_rp1/2_trajectories/0_initial_training/0_initial_trajectory_reacher_df.csv'     # Update directory location 4\n",
        "# Save the DataFrame to Google Drive as a CSV file\n",
        "trajectory_df.to_csv(trajectory_df_path, index=False)"
      ],
      "metadata": {
        "id": "WxoyoPVSXLBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the processed dataframe as a pickle file\n",
        "trajectory_df.to_pickle('/content/drive/MyDrive/data3_rp1/2_trajectories/0_initial_training/0_initial_trajectory_reacher_df.pkl')    # Update directory location 5"
      ],
      "metadata": {
        "id": "N8vZjhXGXVff"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}