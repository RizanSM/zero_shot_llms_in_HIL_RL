{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPwJqkonYGac70N0yyJgYPo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RizanSM/zero_shot_llms_in_HIL_RL/blob/main/02_reacher_env/06_LLM_HF_BF/01_Policy_Training_LLM_HF_BF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the required libraries in your Google Colab environment\n",
        "!pip install gymnasium[mujoco] mujoco stable-baselines3 ollama -q"
      ],
      "metadata": {
        "id": "6ClXxDewl8Ov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary libraries\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import ollama\n",
        "import matplotlib.animation as animation\n",
        "import re"
      ],
      "metadata": {
        "id": "8t0nOj8Cl-Nx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.interpolate import interp1d\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from google.colab import data_table\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "XIJJXwLrmD8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataframe back from the pickle file\n",
        "trajectory_df = pd.read_pickle('/content/drive/MyDrive/data3_rp1/2_trajectories/1_human_feedback/2_Hf_D_Aggressive_Reacher_df_5.pkl')      # Update directory location 1"
      ],
      "metadata": {
        "id": "I7tw2u8TmHxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the data frame\n",
        "data_table.enable_dataframe_formatter()\n",
        "data_table.DataTable(trajectory_df.head())"
      ],
      "metadata": {
        "id": "2CzQChHfmO5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the data type of each column\n",
        "print(type(trajectory_df['Episode'][0]))\n",
        "print(type(trajectory_df['Timestep'][0]))\n",
        "print(type(trajectory_df['State'][0]))\n",
        "print(type(trajectory_df['Action'][0]))\n",
        "print(type(trajectory_df['Reward'][0]))\n",
        "print(type(trajectory_df['Next State'][0]))"
      ],
      "metadata": {
        "id": "dAzQOOdqmTsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_pca(trajectory_df):\n",
        "    \"\"\"\n",
        "    Apply PCA to reduce trajectory features to 3 principal components.\n",
        "    \"\"\"\n",
        "    # Extract numerical values from state and action\n",
        "    state_columns = [f\"state_{i}\" for i in range(len(trajectory_df[\"State\"].iloc[0]))]\n",
        "    action_columns = [f\"action_{i}\" for i in range(len(trajectory_df[\"Action\"].iloc[0]))]\n",
        "\n",
        "    # Expand state and action columns\n",
        "    state_values = np.vstack(trajectory_df[\"State\"].values)\n",
        "    action_values = np.vstack(trajectory_df[\"Action\"].values)\n",
        "\n",
        "    # Create a new DataFrame with extracted features\n",
        "    feature_df = pd.DataFrame(state_values, columns=state_columns)\n",
        "    action_df = pd.DataFrame(action_values, columns=action_columns)\n",
        "\n",
        "    # Concatenate extracted values with reward-related features\n",
        "    features = [\"Reward\", \"Distance Reward\", \"Smoothness Reward\", \"Effort Reward\", \"Smoothness Weighted Reward\", \"Effort Weighted Reward\", \"Recalibrated Reward\"]\n",
        "    feature_df = pd.concat([feature_df, action_df, trajectory_df[features].reset_index(drop=True)], axis=1)\n",
        "\n",
        "    # Apply PCA\n",
        "    pca = PCA(n_components=3)\n",
        "    pca_features = pca.fit_transform(feature_df)\n",
        "\n",
        "    # Add PCA components to original dataframe\n",
        "    trajectory_df_pca = trajectory_df.copy()\n",
        "    trajectory_df_pca[\"PC1\"] = pca_features[:, 0]\n",
        "    trajectory_df_pca[\"PC2\"] = pca_features[:, 1]\n",
        "    trajectory_df_pca[\"PC3\"] = pca_features[:, 2]\n",
        "\n",
        "    return trajectory_df_pca, pca"
      ],
      "metadata": {
        "id": "oEkDeK-sacKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_optimal_clusters(data, max_clusters=10):\n",
        "    \"\"\"\n",
        "    Determine the optimal number of clusters using the Silhouette Score.\n",
        "    \"\"\"\n",
        "    best_score = -1\n",
        "    best_k = 2\n",
        "    for k in range(2, max_clusters + 1):\n",
        "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "        cluster_labels = kmeans.fit_predict(data)\n",
        "        score = silhouette_score(data, cluster_labels)\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_k = k\n",
        "    return best_k"
      ],
      "metadata": {
        "id": "Hk-qpRc-I04l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_clustering(trajectory_df_pca):\n",
        "    \"\"\"\n",
        "    Apply KMeans clustering to select representative timesteps for LLM evaluation.\n",
        "    \"\"\"\n",
        "    features = [\"PC1\", \"PC2\", \"PC3\"]\n",
        "    X = trajectory_df_pca[features]\n",
        "    pca_features = X\n",
        "    # optimal_clusters = find_optimal_clusters(pca_features)\n",
        "    optimal_clusters = 10\n",
        "    print(f\"Optimal number of clusters: {optimal_clusters}\")\n",
        "    kmeans = KMeans(n_clusters=optimal_clusters, random_state=42, n_init=10)\n",
        "    trajectory_df_pca[\"Cluster\"] = kmeans.fit_predict(trajectory_df_pca[[\"PC1\", \"PC2\", \"PC3\"]])\n",
        "\n",
        "    # Select one representative per cluster\n",
        "    cluster_representatives = trajectory_df_pca.groupby(\"Cluster\").first().reset_index()\n",
        "\n",
        "    return trajectory_df_pca, cluster_representatives, kmeans"
      ],
      "metadata": {
        "id": "1kkgHlYlacG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_clusters(trajectory_df_pca):\n",
        "    \"\"\"\n",
        "    Visualize PCA-clustered data in a 3D scatter plot.\n",
        "    \"\"\"\n",
        "    fig = plt.figure(figsize=(10, 7))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "    scatter = ax.scatter(\n",
        "        trajectory_df_pca[\"PC1\"],\n",
        "        trajectory_df_pca[\"PC2\"],\n",
        "        trajectory_df_pca[\"PC3\"],\n",
        "        c=trajectory_df_pca[\"Cluster\"], cmap='viridis', alpha=0.6\n",
        "    )\n",
        "    plt.colorbar(scatter, label=\"Cluster ID\")\n",
        "    ax.set_xlabel(\"PC1\")\n",
        "    ax.set_ylabel(\"PC2\")\n",
        "    ax.set_zlabel(\"PC3\")\n",
        "    plt.title(\"3D PCA-Clustering Visualization\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "nLr-5jIRacEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trajectory_df_pca_1, pca = apply_pca(trajectory_df)"
      ],
      "metadata": {
        "id": "YQ8fmGWgcZik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the data frame\n",
        "data_table.enable_dataframe_formatter()\n",
        "data_table.DataTable(trajectory_df_pca_1.head())"
      ],
      "metadata": {
        "id": "XJgNEYEQoqwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trajectory_df_pca, cluster_representatives, kmeans = apply_clustering(trajectory_df_pca_1)"
      ],
      "metadata": {
        "id": "3HBy-Lq-cg3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the data frame\n",
        "data_table.enable_dataframe_formatter()\n",
        "data_table.DataTable(trajectory_df_pca)"
      ],
      "metadata": {
        "id": "MWt3sfsxp3Hm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(cluster_representatives)\n",
        "print(kmeans)"
      ],
      "metadata": {
        "id": "vbgbxg80MNYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_clusters(trajectory_df_pca)"
      ],
      "metadata": {
        "id": "gSDjpBTzcj_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "B: LLM FEEDBACK IMPLEMENTATION <br>\n",
        "SECTION B.0: LOADING THE LLM\n",
        "*   Step B.0.1: Install Required Libraries\n",
        "*   Step B.0.2: Setting the environment varaible  \n",
        "*   Step B.0.3: Setup and Load the Pretrained LLM"
      ],
      "metadata": {
        "id": "sN6L7NhYmyQq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colab-xterm"
      ],
      "metadata": {
        "id": "swsaDX6QfvFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext colabxterm"
      ],
      "metadata": {
        "id": "HlPT9JHgeLEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%xterm\n",
        "# curl -fsSL https://ollama.com/install.sh | sh\n",
        "# ollama serve & ollama pull mistral\n",
        "# ollama list\n",
        "# ollama show mistral llama3.2"
      ],
      "metadata": {
        "id": "XsP5_8y7eRn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_feedback_progress(progress):\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.set_xlim(0, 100)\n",
        "    ax.set_ylim(0, 1)\n",
        "    ax.set_xlabel(\"Progress (%)\")\n",
        "    ax.set_ylabel(\"Completion\")\n",
        "\n",
        "    def update(frame):\n",
        "        ax.clear()\n",
        "        ax.barh([\"LLM Feedback\"], [frame], color='blue')\n",
        "        ax.set_xlim(0, 100)\n",
        "        ax.set_title(\"Live LLM Feedback Collection Progress\")\n",
        "\n",
        "    ani = animation.FuncAnimation(fig, update, frames=progress, repeat=False)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "F_e3kZZ9pTCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_llm_feedback(state, action, reward, next_state, episode_num, time_step, distance_reward, smoothness_reward, effort_reward, smoothness_weighted_reward, effort_weighted_reward, recalibrated_reward, pc1, pc2, pc3):\n",
        "    \"\"\"\n",
        "    Function to get LLM feedback on whether the recalibrated reward is correct or biased.\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"\n",
        "    You are an expert in reinforcement learning and robotic control environments.\n",
        "    You will analyze agent trajectories from the Reacher-v5 environment and assess whether the recalibrated reward\n",
        "    accurately reflects the agent's performance in reaching and stabilizing the target.\n",
        "\n",
        "    Data details:\n",
        "      Episode: {episode_num}\n",
        "      Time Step: {time_step}\n",
        "      State: {state}\n",
        "      Action: {action}\n",
        "      Reward: {reward}\n",
        "      Next State: {next_state}\n",
        "      Distance Reward: {distance_reward}\n",
        "      Smoothness Reward: {smoothness_reward}\n",
        "      Effort Reward: {effort_reward}\n",
        "      Smoothness Weighted Reward: {smoothness_weighted_reward}\n",
        "      Effort Weighted Reward: {effort_weighted_reward}\n",
        "      Recalibrated Reward: {recalibrated_reward}\n",
        "      PC1: {pc1} (Captures trajectory efficiency and distance control)\n",
        "      PC2: {pc2} (Emphasizes smoothness and effort optimization)\n",
        "      PC3: {pc3} (Represents overall stability and control)\n",
        "\n",
        "    Your task:\n",
        "     - If the recalibrated reward correctly reflects the agent's performance, respond with:\n",
        "          \"Correct reward allotted\"\n",
        "          llm_reward = {recalibrated_reward}\n",
        "\n",
        "     - If the recalibrated reward is biased, respond with:\n",
        "         \"Biased reward allotted\"\n",
        "         llm_reward = Suggested appropriate reward based on reinforcement learning principles.\n",
        "    \"\"\"\n",
        "\n",
        "    answer = ollama.generate(model=\"mistral\", prompt=prompt, options={\"temperature\": 0.6, \"seed\": 4})\n",
        "    feedback = answer['response'].strip()\n",
        "    return feedback"
      ],
      "metadata": {
        "id": "teWEf_0WiJ7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collect_llm_feedback_cluster(cluster_representatives):\n",
        "    llm_feedback_data = {}\n",
        "    for _, row in tqdm(cluster_representatives.iterrows(), total=len(cluster_representatives)):\n",
        "        feedback = get_llm_feedback(\n",
        "            row[\"State\"], row[\"Action\"], row[\"Reward\"], row[\"Next State\"],\n",
        "            row[\"Episode\"], row[\"Timestep\"], row[\"Distance Reward\"], row[\"Smoothness Reward\"], row[\"Effort Reward\"],\n",
        "            row[\"Smoothness Weighted Reward\"], row[\"Effort Weighted Reward\"], row[\"Recalibrated Reward\"],\n",
        "            row[\"PC1\"], row[\"PC2\"], row[\"PC3\"]\n",
        "        )\n",
        "        llm_feedback_data[row[\"Cluster\"]] = feedback\n",
        "    return llm_feedback_data"
      ],
      "metadata": {
        "id": "gbzvQ3GppZLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def interpolate_llm_scores(trajectory_df_pca, llm_feedback_data):\n",
        "    \"\"\"\n",
        "    Interpolates LLM feedback scores across all timesteps.\n",
        "    \"\"\"\n",
        "    trajectory_df_pca[\"LLM_Recalibrated_Reward\"] = trajectory_df_pca[\"Cluster\"].map(llm_feedback_data)\n",
        "    trajectory_df_pca[\"LLM_Recalibrated_Reward\"] = trajectory_df_pca[\"LLM_Recalibrated_Reward\"].interpolate()\n",
        "    return trajectory_df_pca"
      ],
      "metadata": {
        "id": "tXMONDHxisQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_feedback_data = collect_llm_feedback_cluster(cluster_representatives)\n"
      ],
      "metadata": {
        "id": "WXyG474IetsF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trajectory_df_pca = interpolate_llm_scores(trajectory_df_pca, llm_feedback_data)"
      ],
      "metadata": {
        "id": "M37opHA7e0Vb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_table.enable_dataframe_formatter()\n",
        "data_table.DataTable(trajectory_df_pca)"
      ],
      "metadata": {
        "id": "Z6pCtzlxjBMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group the data by 'episode'\n",
        "episode_data = trajectory_df_pca.groupby('Episode')\n",
        "# Loop through each episode\n",
        "for Episode, data in episode_data:\n",
        "\n",
        "    cluster_list = data['Cluster'].tolist()\n",
        "\n",
        "    # Count the total number of time steps in the episode\n",
        "    total_timesteps = data['Timestep'].max() + 1  # Assuming time_step starts from 0\n",
        "\n",
        "    print(f\"Episode {Episode}:Total timesteps {total_timesteps}: {cluster_list}\")"
      ],
      "metadata": {
        "id": "fYtAL3RujBIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Section B.2: REWARD MODELLING(LLM FEEDBACK)\n",
        "\n",
        "*   Step B.2.1: Recalibrate Reward Based on LLM Feedback\n",
        "*   Step 3.2.2: Displaying the recalibrated rewards based on LLM feedback\n",
        "*   Step 3.2.3: Access the reward for a specific step"
      ],
      "metadata": {
        "id": "tSMYBxQNT2Nr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_llm_scores(trajectory_df: pd.DataFrame, trajectory_df_pca: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Extracts llm_score values from the 'LLM_Adjusted_Score' column in trajectory_df_pca and adds them\n",
        "    to a copy of trajectory_df as two new columns: 'LLM_feedback_score' and 'LLM_score'.\n",
        "\n",
        "    Args:\n",
        "        trajectory_df (pd.DataFrame): The original trajectory data.\n",
        "        trajectory_df_pca (pd.DataFrame): Data containing the 'LLM_Adjusted_Score' column with feedback summaries.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Updated DataFrame with extracted LLM scores.\n",
        "    \"\"\"\n",
        "\n",
        "    def extract_llm_score(summary: str) -> float:\n",
        "        match = re.search(r'llm_reward\\s*=\\s*([-+]?[0-9]*\\.?[0-9]+)', str(summary))\n",
        "        return float(match.group(1)) if match else None\n",
        "\n",
        "    updated_df = trajectory_df.copy()\n",
        "    updated_df[\"LLM_feedback_reward\"] = trajectory_df_pca[\"LLM_Recalibrated_Reward\"].apply(\n",
        "        lambda x: f\"llm_reward={extract_llm_score(x)}\" if extract_llm_score(x) is not None else None\n",
        "    )\n",
        "    updated_df[\"LLM_reward\"] = trajectory_df_pca[\"LLM_Recalibrated_Reward\"].apply(extract_llm_score)\n",
        "\n",
        "    # Replace NaN values in LLM_score with zero\n",
        "    updated_df[\"LLM_reward\"].fillna(0, inplace=True)\n",
        "\n",
        "    return updated_df"
      ],
      "metadata": {
        "id": "wkXYfTq6jw75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_feedback_df = extract_llm_scores(trajectory_df, trajectory_df_pca)"
      ],
      "metadata": {
        "id": "6Xe2mairjw4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_table.enable_dataframe_formatter()\n",
        "data_table.DataTable(llm_feedback_df)"
      ],
      "metadata": {
        "id": "BhZN8DGLzlmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group the data by 'episode'\n",
        "episode_data = llm_feedback_df.groupby('Episode')\n",
        "# Loop through each episode\n",
        "for episode, data in episode_data:\n",
        "    # Extract lane indices\n",
        "    llm_score_list = data['LLM_reward'].tolist()\n",
        "\n",
        "    # Count the total number of time steps in the episode\n",
        "    total_timesteps = data['Timestep'].max() + 1  # Assuming time_step starts from 0\n",
        "\n",
        "    print(f\"Episode {Episode}:Total timesteps {total_timesteps}: {llm_score_list}\")"
      ],
      "metadata": {
        "id": "iQlg0w3_jw1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step B.2.3: Recalibrate Reward Based on LLM Feedback\n",
        "# Function to recalibrate the rewards\n",
        "def recalibrate_rewards(df):\n",
        "    # Create a copy of the dataframe\n",
        "    df_copy = df.copy()\n",
        "\n",
        "    alpha = -1\n",
        "    # Create the 'Recalibrated_rewards' column\n",
        "    df_copy['New_Recalibrated_rewards'] = df_copy['Reward'] + alpha * (df_copy['LLM_reward'])\n",
        "\n",
        "    # Get the list of recalibrated rewards\n",
        "    new_recalibrated_rewards_list = df_copy['New_Recalibrated_rewards'].tolist()\n",
        "\n",
        "    return df_copy, new_recalibrated_rewards_list"
      ],
      "metadata": {
        "id": "lBcjhCzAjwpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the function to recalibrate rewards\n",
        "recalibrated_df, recalibrated_rewards_list = recalibrate_rewards(llm_feedback_df)"
      ],
      "metadata": {
        "id": "VWEyihqymU-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recalibrated_df.to_pickle('/content/drive/MyDrive/data3_rp1/2_trajectories/3_llm_hf_bf/1_llm_hd_bf_df_reacher_5.pkl')      # Update directory location 2"
      ],
      "metadata": {
        "id": "ak-TLG0Jmu22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To access the reward for a specific step:\n",
        "for i, feedback in enumerate(recalibrated_rewards_list):\n",
        "    human_recalibrated_reward_for_step = recalibrated_rewards_list[i]\n",
        "    print(f\"Recalibrated reward for step {i}: {human_recalibrated_reward_for_step}\")"
      ],
      "metadata": {
        "id": "dDuOe0VsmU5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SECTION A.5: MODEL TRAINING(HUMAN FEEDBACK DIRECT- IDEAL CASE SCENARIO)\n",
        "*   Step A.5.1: CUSTOM REWARD FUNCTION\n",
        "*   Step A.5.2: LOAD THE SAVED INITIALLY TRAINED PPO MODEL FROM GOOGLE DRIVE\n",
        "*   Step A.5.3: TRAIN/UPDATE PPO MODEL WITH RECALIBRATED REWARD\n",
        "*   Step A.5.4: SAVE THE TRAINED MODEL(HF_IDEAL) FOR TESTING"
      ],
      "metadata": {
        "id": "hFSLS54jm_yE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step A.5.1: CUSTOM REWARD FUNCTION\n",
        "def custom_reward(self, env, state, action, next_state, reward, done):\n",
        "    # Access and recalculate the reward using human_feedback_data or recalibrate_rewards_human function\n",
        "    global step_counter\n",
        "    try:\n",
        "        step_counter\n",
        "    except NameError:\n",
        "        step_counter = 0\n",
        "\n",
        "    reward = recalibrated_rewards_list[step_counter]\n",
        "    step_counter += 1\n",
        "    return reward\n",
        "\n",
        "# Create a new environment class that wraps your original environment and overrides the default reward function with your custom function\n",
        "class CustomRewardWrapper(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        super(CustomRewardWrapper, self).__init__(env)\n",
        "\n",
        "    def step(self, action):\n",
        "        next_state, reward, terminated, truncated, info = self.env.step(action)\n",
        "        done = terminated or truncated\n",
        "        reward = custom_reward(self, self.env, self.last_obs, action, next_state, reward, done)\n",
        "        # custom_reward should be defined and accessible to your class\n",
        "        self.last_obs = next_state\n",
        "        return next_state, reward, terminated, truncated, info\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        global step_counter\n",
        "        step_counter = 0\n",
        "        self.last_obs = self.env.reset(**kwargs)[0]  # Assuming Gymnasium env returns (obs, info)\n",
        "        return self.last_obs, {}  # Assuming Gymnasium env requires (obs, info)\n",
        "# Create and wrap the environment with your custom reward wrapper\n",
        "# env_human = CustomRewardWrapper(gym.make('highway-v0'))"
      ],
      "metadata": {
        "id": "AqogGzpsmU07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PPO training and Training logs"
      ],
      "metadata": {
        "id": "6-pv1yeKnGSo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive_log_dir = \"/content/drive/MyDrive/data3_rp1/0_log_dir/5_ppo_reacher_llm_hf_bf_8\"          # Update directory location 3"
      ],
      "metadata": {
        "id": "qeN5ejwRmUzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train PPO with Custom Rewards\n",
        "def train_ppo_with_custom_rewards(log_dir=drive_log_dir, total_timesteps=200000):\n",
        "    os.makedirs(log_dir, exist_ok=True)\n",
        "    env = CustomRewardWrapper(gym.make(\"Reacher-v5\"))\n",
        "    env = Monitor(env, log_dir)\n",
        "    model = PPO(\"MlpPolicy\", env, verbose=1, tensorboard_log=log_dir)\n",
        "    model.learn(total_timesteps=total_timesteps)\n",
        "    model.save('/content/drive/MyDrive/data3_rp1/1_trained_models/5_ppo_reacher_llm_hf_bf_8')   # Update directory location 4\n",
        "    return model, log_dir"
      ],
      "metadata": {
        "id": "jXmD4dl8mUw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Execute Training and Convergence Tracking\n",
        "model, log_dir = train_ppo_with_custom_rewards(total_timesteps=200000)"
      ],
      "metadata": {
        "id": "62lnvdO-nPs3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_path = os.path.join(drive_log_dir, \"monitor.csv\")\n",
        "df = pd.read_csv(log_path, skiprows=1)\n",
        "# Ensure episodes are logged correctly\n",
        "df.reset_index(inplace=True)\n",
        "df.rename(columns={\"index\": \"episode\", \"r\": \"reward\", \"l\": \"length\", \"t\": \"time_step\"}, inplace=True)"
      ],
      "metadata": {
        "id": "a5eOE4sfnP2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_table.enable_dataframe_formatter()\n",
        "data_table.DataTable(df.head())"
      ],
      "metadata": {
        "id": "yDjmFtMsnPyy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}