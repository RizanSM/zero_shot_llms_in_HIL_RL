{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMMa3qhmtSD0NhW9RvDJOT5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RizanSM/zero_shot_llms_in_HIL_RL/blob/main/02_reacher_env/03_BIASED_HF_D_AGG/01_Policy_Training_BIASED_HF_D_AGG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium[mujoco] mujoco stable-baselines3 -q"
      ],
      "metadata": {
        "id": "APrU5xhT0nTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary libraries\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import os\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "SQJ0fTEA0m_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from google.colab import data_table\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "go9u8BAE1Olz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataframe back from the pickle file\n",
        "trajectory_df = pd.read_pickle('/content/drive/MyDrive/data3_rp1/2_trajectories/0_initial_training/0_initial_trajectory_reacher_df.pkl')      # Update directory location 1"
      ],
      "metadata": {
        "id": "84qr9aP-nc0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the data frame\n",
        "data_table.enable_dataframe_formatter()\n",
        "data_table.DataTable(trajectory_df)"
      ],
      "metadata": {
        "id": "MHAY3C2LxMel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the data type of each column\n",
        "print(type(trajectory_df['Episode'][0]))\n",
        "print(type(trajectory_df['Timestep'][0]))\n",
        "print(type(trajectory_df['State'][0]))\n",
        "print(type(trajectory_df['Action'][0]))\n",
        "print(type(trajectory_df['Reward'][0]))\n",
        "print(type(trajectory_df['Next State'][0]))"
      ],
      "metadata": {
        "id": "BPFidEkGlZAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trajectory_df.dtypes"
      ],
      "metadata": {
        "id": "NYe0lzD3XNwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "HUMAN FEEDBACK IMPLEMENTATION (IDEAL CASE SCENARIO)\n",
        "\n"
      ],
      "metadata": {
        "id": "yK3Yd5Tt2_FU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_different_rewards(trajectory_df):\n",
        "    \"\"\"Calculates effort-based reward and returns updated dataframe.\"\"\"\n",
        "    df = trajectory_df.copy()\n",
        "\n",
        "    df['Distance Reward'] = -df['State'].apply(lambda x: np.linalg.norm(np.array(x)[8:10], axis=0))\n",
        "\n",
        "    df['Smoothness Reward'] = -np.abs(df['State'].apply(lambda x: np.array(x)[6:8]).apply(np.linalg.norm))\n",
        "\n",
        "    # df['Effort Reward'] = -df['Action'].apply(lambda x: np.linalg.norm(np.array(x)**2, axis=0))\n",
        "    df['Effort Reward'] = -df['Action'].apply(lambda x: np.linalg.norm(np.array(x), axis=0))\n",
        "    return df"
      ],
      "metadata": {
        "id": "CYqGqXvErLD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "different_rewards_df = calculate_different_rewards(trajectory_df)"
      ],
      "metadata": {
        "id": "D4WdqH8mwzGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_table.enable_dataframe_formatter()\n",
        "data_table.DataTable(different_rewards_df)"
      ],
      "metadata": {
        "id": "O7sP1CjCwzCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_weighted_rewards(df):\n",
        "    # Step 1: Create a copy of the dataframe\n",
        "    df = df.copy()\n",
        "\n",
        "    # Step 2: Compute min and max values for Smoothness Reward and Effort Reward\n",
        "    min_smoothness = df[\"Smoothness Reward\"].min()\n",
        "    max_smoothness = df[\"Smoothness Reward\"].max()\n",
        "\n",
        "    min_effort = df[\"Effort Reward\"].min()\n",
        "    max_effort = df[\"Effort Reward\"].max()\n",
        "\n",
        "    # Step 3: Compute Smoothness Score Range and Effort Score Range\n",
        "    xsmooth = 1 / abs(max_smoothness - min_smoothness)\n",
        "    xeffort = 1 / abs(max_effort - min_effort)\n",
        "\n",
        "    print(f\"Smoothness Score Range: {xsmooth}\")\n",
        "    print(f\"Effort Score Range: {xeffort}\")\n",
        "\n",
        "    # Step 4: Compute Smoothness Weight (λ) and Effort Weight (ε) using Softmax\n",
        "    exp_xsmooth = np.exp(xsmooth)\n",
        "    exp_xeffort = np.exp(xeffort)\n",
        "\n",
        "    lambda_smooth = exp_xsmooth / (exp_xsmooth + exp_xeffort)\n",
        "    epsilon_effort = exp_xeffort / (exp_xsmooth + exp_xeffort)\n",
        "\n",
        "    # Adjust weights based on bias type\n",
        "    # bias_type == \"aggressive\":\n",
        "    lambda_smooth *= 0  # Reduce smoothness importance                          # bias value 1\n",
        "    epsilon_effort *= 2  # Increase effort tolerance                            # bias value 2\n",
        "\n",
        "    print(f\"Smoothness Weight (λ): {lambda_smooth}\")\n",
        "    print(f\"Effort Weight (ε): {epsilon_effort}\")\n",
        "\n",
        "    # Step 5: Compute Smoothness Weighted Reward (R1) and Effort Weighted Reward (R2)\n",
        "    df[\"Smoothness Weighted Reward\"] = -df[\"Distance Reward\"] - lambda_smooth * df[\"Smoothness Reward\"]\n",
        "    df[\"Effort Weighted Reward\"] = -df[\"Distance Reward\"] - epsilon_effort * df[\"Effort Reward\"]\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "dDvuQt-oIZ9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reward_df = compute_weighted_rewards(different_rewards_df)"
      ],
      "metadata": {
        "id": "LCW5eE4YWsCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_table.enable_dataframe_formatter()\n",
        "data_table.DataTable(reward_df)"
      ],
      "metadata": {
        "id": "MfFmHUkPWr_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to recalibrate the rewards\n",
        "def recalibrate_rewards(df):\n",
        "    # Create a copy of the dataframe\n",
        "    df_copy = df.copy()\n",
        "\n",
        "    alpha = -1\n",
        "    # Create the 'Recalibrated_rewards' column\n",
        "    df_copy['Recalibrated Reward'] = df_copy['Reward'] + alpha * (df_copy['Smoothness Weighted Reward'] + df_copy['Effort Weighted Reward'])\n",
        "\n",
        "    # Get the list of recalibrated rewards\n",
        "    recalibrated_rewards_list = df_copy['Recalibrated Reward'].tolist()\n",
        "\n",
        "    return df_copy, recalibrated_rewards_list"
      ],
      "metadata": {
        "id": "vidiDsOYWr72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the function to recalibrate rewards\n",
        "recalibrated_df, recalibrated_rewards_list = recalibrate_rewards(reward_df)"
      ],
      "metadata": {
        "id": "53_Vxoy_Wr6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_table.enable_dataframe_formatter()\n",
        "data_table.DataTable(recalibrated_df)"
      ],
      "metadata": {
        "id": "S4Upp_JDWr4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recalibrated_df.to_pickle('/content/drive/MyDrive/data3_rp1/2_trajectories/1_human_feedback/2_Hf_D_Aggressive_Reacher_df_5.pkl')         # Update directory location 2"
      ],
      "metadata": {
        "id": "Vt7u-fySWr16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To access the reward for a specific step:\n",
        "for i, feedback in enumerate(recalibrated_rewards_list):\n",
        "    human_recalibrated_reward_for_step = recalibrated_rewards_list[i]\n",
        "    print(f\"Recalibrated reward for step {i}: {human_recalibrated_reward_for_step}\")"
      ],
      "metadata": {
        "id": "8he0GVnEb9gN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step A.5.1: CUSTOM REWARD FUNCTION\n",
        "def custom_reward(self, env, state, action, next_state, reward, done):\n",
        "    # Access and recalculate the reward using human_feedback_data or recalibrate_rewards_human function\n",
        "    global step_counter\n",
        "    try:\n",
        "        step_counter\n",
        "    except NameError:\n",
        "        step_counter = 0\n",
        "\n",
        "    reward = recalibrated_rewards_list[step_counter]\n",
        "    step_counter += 1\n",
        "    return reward\n",
        "\n",
        "# Create a new environment class that wraps your original environment and overrides the default reward function with your custom function\n",
        "class CustomRewardWrapper(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        super(CustomRewardWrapper, self).__init__(env)\n",
        "\n",
        "    def step(self, action):\n",
        "        next_state, reward, terminated, truncated, info = self.env.step(action)\n",
        "        done = terminated or truncated\n",
        "        reward = custom_reward(self, self.env, self.last_obs, action, next_state, reward, done)\n",
        "        # custom_reward should be defined and accessible to your class\n",
        "        self.last_obs = next_state\n",
        "        return next_state, reward, terminated, truncated, info\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        global step_counter\n",
        "        step_counter = 0\n",
        "        self.last_obs = self.env.reset(**kwargs)[0]  # Assuming Gymnasium env returns (obs, info)\n",
        "        return self.last_obs, {}  # Assuming Gymnasium env requires (obs, info)\n",
        "# Create and wrap the environment with your custom reward wrapper\n",
        "# env_human = CustomRewardWrapper(gym.make('highway-v0'))"
      ],
      "metadata": {
        "id": "hkyUKppsb9di"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PPO training and Training logs"
      ],
      "metadata": {
        "id": "LX74c1zjfa8H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive_log_dir = \"/content/drive/MyDrive/data3_rp1/0_log_dir/2_ppo_reacher_hf_direct_aggressive_8\"              # Update directory location 3"
      ],
      "metadata": {
        "id": "sPEnlY_Ib9br"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train PPO with Custom Rewards\n",
        "def train_ppo_with_custom_rewards(log_dir=drive_log_dir, total_timesteps=200000):\n",
        "    os.makedirs(log_dir, exist_ok=True)\n",
        "    env = CustomRewardWrapper(gym.make(\"Reacher-v5\"))\n",
        "    env = Monitor(env, log_dir)\n",
        "    model = PPO(\"MlpPolicy\", env, verbose=1, tensorboard_log=log_dir)\n",
        "    model.learn(total_timesteps=total_timesteps)\n",
        "    model.save('/content/drive/MyDrive/data3_rp1/1_trained_models/2_ppo_reacher_hf_direct_aggressive_8')       # Update directory location 4\n",
        "    return model, log_dir"
      ],
      "metadata": {
        "id": "OhKJzaBLb9Zo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Execute Training and Convergence Tracking\n",
        "model, log_dir = train_ppo_with_custom_rewards(total_timesteps=200000)"
      ],
      "metadata": {
        "id": "9dXhkNClb9Xz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_path = os.path.join(drive_log_dir, \"monitor.csv\")\n",
        "df = pd.read_csv(log_path, skiprows=1)\n",
        "# Ensure episodes are logged correctly\n",
        "df.reset_index(inplace=True)\n",
        "df.rename(columns={\"index\": \"episode\", \"r\": \"reward\", \"l\": \"length\", \"t\": \"time_step\"}, inplace=True)"
      ],
      "metadata": {
        "id": "74j8sUOLwWrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_table.enable_dataframe_formatter()\n",
        "data_table.DataTable(df.head())"
      ],
      "metadata": {
        "id": "rdVGfuWEB0Kq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}