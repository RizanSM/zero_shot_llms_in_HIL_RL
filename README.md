# zero_shot_llms_in_HIL_RL
This work explores replacing human feedback in reinforcement learning with zero-shot large language models (LLMs) for reward shaping. We propose a hybrid framework (LLM-HFBF) that not only provides feedback but also flags and corrects bias in human input, boosting policy performance in continuous control tasks.
