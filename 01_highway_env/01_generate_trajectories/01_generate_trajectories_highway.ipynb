{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RizanSM/zero_shot_llms_in_HIL_RL/blob/main/01_highway_env/01_generate_trajectories/01_generate_trajectories_highway.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ghTtKSy6cKA"
      },
      "outputs": [],
      "source": [
        "# Install the required libraries in your Google Colab environment\n",
        "!pip install stable-baselines3 gymnasium highway-env -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yuugqNVe72lS"
      },
      "outputs": [],
      "source": [
        "# Import the necessary libraries\n",
        "import gymnasium as gym\n",
        "import highway_env\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import os\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LzgFnH49-xCA"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from google.colab import data_table\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to save the training logs\n",
        "log_dir = \"/content/drive/MyDrive/data_rp1/0_log_dir/0_ppo_highway_initial\"                     # Update directory location 1\n",
        "os.makedirs(log_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "CkEXB854F52_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58a8K2QD74at"
      },
      "outputs": [],
      "source": [
        "# THE ENVIRONMENT\n",
        "# Step 1.1: Choose the Environment\n",
        "# Initialize the environment.\n",
        "env = gym.make('highway-v0')\n",
        "env = Monitor(env, log_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fgya8Jce8Vx9"
      },
      "outputs": [],
      "source": [
        "# Step 1.2: Initial Observation\n",
        "# Print out a sample observation to see what the agent receives at the start.\n",
        "obs = env.reset()\n",
        "print(\"Initial Observation: \", obs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZUGe8Swt8ez0"
      },
      "outputs": [],
      "source": [
        "# Step 1.3: Implement PPO with the Highway environment\n",
        "# Initialize the PPO algorithm with the chosen environment.\n",
        "# Create the PPO model with the Highway environment\n",
        "model = PPO('MlpPolicy', env, verbose=1, tensorboard_log=log_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8tqwuvW8hmE"
      },
      "outputs": [],
      "source": [
        "# Step 1.4: Train the model\n",
        "# 10,000 timesteps for initial training\n",
        "model.learn(total_timesteps=10000)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "log_path = os.path.join(log_dir, \"monitor.csv\")\n",
        "df = pd.read_csv(log_path, skiprows=1)\n",
        "df.rename(columns={\"index\": \"episode\", \"r\": \"reward\", \"l\": \"length\", \"t\": \"time_step\"}, inplace=True)"
      ],
      "metadata": {
        "id": "i6fmwnePGy7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zn_sn-rP8woi"
      },
      "outputs": [],
      "source": [
        "# Step 1.5: Save the trained model to Google Drive\n",
        "model.save('/content/drive/MyDrive/data_rp1/1_trained_models/0_ppo_highway_model_with_intial_training')                   # Update directory location 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qK4Ef2zP8zbb"
      },
      "outputs": [],
      "source": [
        "# Step 1.6: Load the saved PPO model from Google Drive\n",
        "model = PPO.load('/content/drive/MyDrive/data_rp1/1_trained_models/0_ppo_highway_model_with_intial_training')             # Update directory location 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8ccvEreBzGF"
      },
      "source": [
        "# TRAJECTORY GENERATION AND COLLECTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZ0Ok3RR80YW"
      },
      "outputs": [],
      "source": [
        "# Step-2: TRAJECTORY COLLECTION\n",
        "# TRAJECTORY COLLECTION WITH ADDTIONNAL INFORMATION\n",
        "# Initialize a list to store trajectory data\n",
        "trajectories = []\n",
        "\n",
        "# FUNCTION TO COLLECT TRAJECTORY DATA (state-action-reward transitions).\n",
        "\n",
        "def collect_trajectory_data(env, model, num_episodes):\n",
        "    \"\"\"\n",
        "    Collect trajectory data for a number of episodes.\n",
        "    Each trajectory contains state-action-reward sequences.\n",
        "    \"\"\"\n",
        "    trajectory_data = []\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state, _ = env.reset()  # Reset the environment at the start of each episode\n",
        "        done = False\n",
        "        episode_data = []\n",
        "\n",
        "        while not done:\n",
        "            # Get action from the trained PPO model\n",
        "            action, _states = model.predict(state)\n",
        "\n",
        "            # Take the action and get next state and reward\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            # Extract lane index and collision flag\n",
        "            lane_index = int(env.unwrapped.vehicle.lane_index[2])  # Assuming 'env.unwrapped.vehicle' gives access to the agent's vehicle object and 'lane_index' attribute within it contains the lane index.\n",
        "            collision_flag = int(info.get('crashed', 0))  # Use get() with a default value to handle missing 'crashed' key.\n",
        "\n",
        "            # Store the trajectory: (state, action, reward, next_state)\n",
        "            episode_data.append({\n",
        "                \"state\": state,\n",
        "                \"action\": action,\n",
        "                \"reward\": reward,\n",
        "                \"next_state\": next_state,\n",
        "                \"lane_indices\": lane_index,\n",
        "                \"collision_flags\": collision_flag\n",
        "            })\n",
        "\n",
        "            # Update the state for the next iteration\n",
        "            state = next_state\n",
        "\n",
        "        # Add the episode data to the overall trajectory list\n",
        "        trajectory_data.append(episode_data)\n",
        "\n",
        "    return trajectory_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1-6QG4089_4"
      },
      "outputs": [],
      "source": [
        "# Collect data for 100 episodes\n",
        "trajectory_data = collect_trajectory_data(env, model, num_episodes=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OqzawCb_-FQW"
      },
      "outputs": [],
      "source": [
        "# FUNCTION TO PREPROCESS TRAJECTORY DATA\n",
        "def preprocess_trajectory_data(trajectory_data):\n",
        "    \"\"\"\n",
        "    Preprocesses the trajectory data into a structured format for further analysis.\n",
        "    Returns a DataFrame with columns: episode, time_step, state, action, reward, next_state, speed, and reward_details.\n",
        "    \"\"\"\n",
        "    processed_data = []\n",
        "\n",
        "    for episode_num, episode_data in enumerate(trajectory_data):\n",
        "        for time_step, step in enumerate(episode_data):\n",
        "            # Flatten the state and next_state for easy interpretation (if they are multi-dimensional)\n",
        "            state = np.array(step['state']).flatten()  # Flatten the state vector (if multi-dimensional)\n",
        "            next_state = np.array(step['next_state']).flatten()  # Flatten the next_state vector\n",
        "\n",
        "            collision_flag = step['collision_flags']\n",
        "            lane_index = step['lane_indices']\n",
        "\n",
        "            # Append the processed data for this step\n",
        "            processed_data.append({\n",
        "                \"episode\": episode_num,\n",
        "                \"time_step\": time_step,\n",
        "                \"state\": state,\n",
        "                \"action\": step['action'],\n",
        "                \"reward\": step['reward'],\n",
        "                \"next_state\": next_state,\n",
        "                \"collision_flag\": collision_flag,\n",
        "                \"lane_index\": lane_index\n",
        "            })\n",
        "\n",
        "    # Convert the list of processed data into a DataFrame\n",
        "    df = pd.DataFrame(processed_data)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5mPeFhs-PBV"
      },
      "outputs": [],
      "source": [
        "# Preprocess the trajectory data\n",
        "trajectory_df = preprocess_trajectory_data(trajectory_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfN51v7_-TbF"
      },
      "outputs": [],
      "source": [
        "data_table.enable_dataframe_formatter()\n",
        "data_table.DataTable(trajectory_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8vBT6pro-61"
      },
      "outputs": [],
      "source": [
        "# Check the data type of each column\n",
        "print(type(trajectory_df['episode'][0]))\n",
        "print(type(trajectory_df['time_step'][0]))\n",
        "print(type(trajectory_df['state'][0]))\n",
        "print(type(trajectory_df['action'][0]))\n",
        "print(type(trajectory_df['reward'][0]))\n",
        "print(type(trajectory_df['next_state'][0]))\n",
        "print(type(trajectory_df['collision_flag'][0]))\n",
        "print(type(trajectory_df['lane_index'][0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDXCkSPHm1_Z"
      },
      "outputs": [],
      "source": [
        "# Define the path to save the DataFrame (adjust the path as necessary)\n",
        "trajectory_df_path = '/content/drive/MyDrive/data_rp1/2_trajectories/0_initial_training/initial_trajectory_df.csv'              # Update directory location 4\n",
        "# Save the DataFrame to Google Drive as a CSV file\n",
        "trajectory_df.to_csv(trajectory_df_path, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xh_cIweonCA7"
      },
      "outputs": [],
      "source": [
        "# Save the processed dataframe as a pickle file\n",
        "trajectory_df.to_pickle('/content/drive/MyDrive/data_rp1/2_trajectories/0_initial_training/0_initial_trajectory_df.pkl')        # Update directory location 5"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPYf1ia6cyP5RocUa73SofZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}