{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPR+CPHesQh2YfUs5DbWcjU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RizanSM/zero_shot_llms_in_HIL_RL/blob/main/01_highway_env/02_default_env/07_LLM_DIRECT/01_Policy_Training_LLM_DIRECT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the required libraries in your Google Colab environment\n",
        "!pip install stable-baselines3 gymnasium highway-env ollama -q"
      ],
      "metadata": {
        "id": "6ClXxDewl8Ov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary libraries\n",
        "import gymnasium as gym\n",
        "import highway_env\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import ollama\n",
        "import matplotlib.animation as animation\n",
        "import re"
      ],
      "metadata": {
        "id": "8t0nOj8Cl-Nx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.interpolate import interp1d\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from google.colab import data_table\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "XIJJXwLrmD8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataframe back from the pickle file\n",
        "trajectory_df = pd.read_pickle('/content/drive/MyDrive/data_rp1/2_trajectories/0_initial_training/0_initial_trajectory_df.pkl')       # Update directory location 1"
      ],
      "metadata": {
        "id": "I7tw2u8TmHxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the data frame\n",
        "data_table.enable_dataframe_formatter()\n",
        "data_table.DataTable(trajectory_df.head())"
      ],
      "metadata": {
        "id": "2CzQChHfmO5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the data type of each column\n",
        "print(type(trajectory_df['episode'][0]))\n",
        "print(type(trajectory_df['time_step'][0]))\n",
        "print(type(trajectory_df['state'][0]))\n",
        "print(type(trajectory_df['action'][0]))\n",
        "print(type(trajectory_df['reward'][0]))\n",
        "print(type(trajectory_df['next_state'][0]))\n",
        "print(type(trajectory_df['collision_flag'][0]))\n",
        "print(type(trajectory_df['lane_index'][0]))"
      ],
      "metadata": {
        "id": "dAzQOOdqmTsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_pca(trajectory_df):\n",
        "    \"\"\"\n",
        "    Apply PCA to reduce trajectory features to 3 principal components.\n",
        "    \"\"\"\n",
        "    # Convert 'state' and 'next_state' columns to numerical features\n",
        "    trajectory_df['state'] = trajectory_df['state'].apply(lambda x: x[1])\n",
        "    trajectory_df['next_state'] = trajectory_df['next_state'].apply(lambda x: x[1])\n",
        "\n",
        "    # Convert 'state' and 'next_state' columns to numerical features\n",
        "    # Check if the element is a list or tuple before indexing\n",
        "    #trajectory_df['state'] = trajectory_df['state'].apply(lambda x: x[0] if isinstance(x, (list, tuple)) else x)\n",
        "    #trajectory_df['next_state'] = trajectory_df['next_state'].apply(lambda x: x[0] if isinstance(x, (list, tuple)) else x)\n",
        "\n",
        "    features = [\"state\", \"action\", \"reward\", \"collision_flag\", \"lane_index\"]\n",
        "    pca = PCA(n_components=3)\n",
        "    X = trajectory_df[features]\n",
        "    pca_features = pca.fit_transform(X)\n",
        "\n",
        "    trajectory_df_pca = trajectory_df.copy()\n",
        "    trajectory_df_pca[\"PC1\"] = pca_features[:, 0]\n",
        "    trajectory_df_pca[\"PC2\"] = pca_features[:, 1]\n",
        "    trajectory_df_pca[\"PC3\"] = pca_features[:, 2]\n",
        "\n",
        "    return trajectory_df_pca, pca"
      ],
      "metadata": {
        "id": "oEkDeK-sacKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_optimal_clusters(data, max_clusters=10):\n",
        "    \"\"\"\n",
        "    Determine the optimal number of clusters using the Silhouette Score.\n",
        "    \"\"\"\n",
        "    best_score = -1\n",
        "    best_k = 2\n",
        "    for k in range(2, max_clusters + 1):\n",
        "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "        cluster_labels = kmeans.fit_predict(data)\n",
        "        score = silhouette_score(data, cluster_labels)\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_k = k\n",
        "    return best_k"
      ],
      "metadata": {
        "id": "Hk-qpRc-I04l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_clustering(trajectory_df_pca):\n",
        "    \"\"\"\n",
        "    Apply KMeans clustering to select representative timesteps for LLM evaluation.\n",
        "    \"\"\"\n",
        "    features = [\"PC1\", \"PC2\", \"PC3\"]\n",
        "    X = trajectory_df_pca[features]\n",
        "    pca_features = X\n",
        "    optimal_clusters = find_optimal_clusters(pca_features)\n",
        "    print(f\"Optimal number of clusters: {optimal_clusters}\")\n",
        "    kmeans = KMeans(n_clusters=optimal_clusters, random_state=42, n_init=10)\n",
        "    trajectory_df_pca[\"Cluster\"] = kmeans.fit_predict(trajectory_df_pca[[\"PC1\", \"PC2\", \"PC3\"]])\n",
        "\n",
        "    # Select one representative per cluster\n",
        "    cluster_representatives = trajectory_df_pca.groupby(\"Cluster\").first().reset_index()\n",
        "\n",
        "    return trajectory_df_pca, cluster_representatives, kmeans"
      ],
      "metadata": {
        "id": "1kkgHlYlacG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_clusters(trajectory_df_pca):\n",
        "    \"\"\"\n",
        "    Visualize PCA-clustered data in a 3D scatter plot.\n",
        "    \"\"\"\n",
        "    fig = plt.figure(figsize=(10, 7))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "    scatter = ax.scatter(\n",
        "        trajectory_df_pca[\"PC1\"],\n",
        "        trajectory_df_pca[\"PC2\"],\n",
        "        trajectory_df_pca[\"PC3\"],\n",
        "        c=trajectory_df_pca[\"Cluster\"], cmap='viridis', alpha=0.6\n",
        "    )\n",
        "    plt.colorbar(scatter, label=\"Cluster ID\")\n",
        "    ax.set_xlabel(\"PC1\")\n",
        "    ax.set_ylabel(\"PC2\")\n",
        "    ax.set_zlabel(\"PC3\")\n",
        "    plt.title(\"3D PCA-Clustering Visualization\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "nLr-5jIRacEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trajectory_df_pca, pca = apply_pca(trajectory_df)"
      ],
      "metadata": {
        "id": "YQ8fmGWgcZik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trajectory_df_pca, cluster_representatives, kmeans = apply_clustering(trajectory_df_pca)"
      ],
      "metadata": {
        "id": "3HBy-Lq-cg3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(cluster_representatives)\n",
        "print(kmeans)"
      ],
      "metadata": {
        "id": "vbgbxg80MNYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_clusters(trajectory_df_pca)"
      ],
      "metadata": {
        "id": "gSDjpBTzcj_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "B: LLM FEEDBACK IMPLEMENTATION <br>\n",
        "SECTION B.0: LOADING THE LLM\n",
        "*   Step B.0.1: Install Required Libraries\n",
        "*   Step B.0.2: Setting the environment varaible  \n",
        "*   Step B.0.3: Setup and Load the Pretrained LLM"
      ],
      "metadata": {
        "id": "sN6L7NhYmyQq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colab-xterm"
      ],
      "metadata": {
        "id": "swsaDX6QfvFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext colabxterm"
      ],
      "metadata": {
        "id": "HlPT9JHgeLEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%xterm\n",
        "# Execute the following commands sequentially in Xterm\n",
        "# curl -fsSL https://ollama.com/install.sh | sh\n",
        "# ollama serve & ollama pull mistral\n",
        "# ollama list\n",
        "# ollama show mistral"
      ],
      "metadata": {
        "id": "XsP5_8y7eRn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_feedback_progress(progress):\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.set_xlim(0, 100)\n",
        "    ax.set_ylim(0, 1)\n",
        "    ax.set_xlabel(\"Progress (%)\")\n",
        "    ax.set_ylabel(\"Completion\")\n",
        "\n",
        "    def update(frame):\n",
        "        ax.clear()\n",
        "        ax.barh([\"LLM Feedback\"], [frame], color='blue')\n",
        "        ax.set_xlim(0, 100)\n",
        "        ax.set_title(\"Live LLM Feedback Collection Progress\")\n",
        "\n",
        "    ani = animation.FuncAnimation(fig, update, frames=progress, repeat=False)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "F_e3kZZ9pTCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_llm_feedback(state, action, reward, next_state, episode_num, time_step, collision_flag, lane_index, pc1, pc2, pc3):\n",
        "    \"\"\"\n",
        "    Query LLM for feedback on action effectiveness and reward appropriateness, incorporating principal components.\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"\n",
        "    You are an expert in analyzing reinforcement learning trajectories in a highway environment.\n",
        "    You will process a structured dataset containing agent trajectories and analyze each row by assessing whether the action taken is effective or not,\n",
        "    and whether the reward awarded is appropriate or not.\n",
        "\n",
        "    You are provided with a trajectory data frame containing the following details:\n",
        "      - Episode: {episode_num}\n",
        "      - Time Step: {time_step}\n",
        "      - State: {state}\n",
        "      - Action taken by agent: {action}\n",
        "      - Reward: {reward}\n",
        "      - Next state: {next_state}\n",
        "      - Collision Flag: {collision_flag}\n",
        "      - Lane Index: {lane_index}\n",
        "      - Principal Components: PC1 = {pc1}, PC2 = {pc2}, PC3 = {pc3}\n",
        "\n",
        "    Your task is to analyze this data and provide feedback based on:\n",
        "    1. Action Effectiveness Evaluation:\n",
        "       a. If the action taken is effective, respond with:\n",
        "          llm_score_1 = +2\n",
        "       b. If the action taken is ineffective, respond with:\n",
        "          llm_score_1 = -2\n",
        "       c. If the action taken has no effect (neither effective nor ineffective), respond with:\n",
        "          llm_score_1 = 0\n",
        "\n",
        "    2. Reward Appropriateness Evaluation:\n",
        "       a. If the reward awarded is appropriate, respond with:\n",
        "          llm_score_2 = +1\n",
        "       b. If the reward awarded is inappropriate, respond with:\n",
        "          llm_score_2 = -1\n",
        "\n",
        "    For each row, your output feedback should strictly follow this format:\n",
        "       - \"Justification for the decision\"\n",
        "       - Action Effectiveness Evaluation: <llm_score_1>\n",
        "       - Reward Appropriateness Evaluation: <llm_score_2>\n",
        "    \"\"\"\n",
        "\n",
        "    answer = ollama.generate(model=\"mistral\", prompt=prompt, options={\"temperature\": 0.6, \"seed\": 4})\n",
        "    feedback = answer['response'].strip()\n",
        "\n",
        "    return feedback"
      ],
      "metadata": {
        "id": "teWEf_0WiJ7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collect_llm_feedback_cluster(cluster_representatives):\n",
        "    llm_feedback_data = {}\n",
        "    progress = []\n",
        "    for i, (_, row) in tqdm(enumerate(cluster_representatives.iterrows()), total=len(cluster_representatives)):\n",
        "        feedback = get_llm_feedback(\n",
        "            row[\"state\"], row[\"action\"], row[\"reward\"], row[\"next_state\"],\n",
        "            row[\"episode\"], row[\"time_step\"], row[\"collision_flag\"], row[\"lane_index\"],\n",
        "            row[\"PC1\"], row[\"PC2\"], row[\"PC3\"]\n",
        "        )\n",
        "        llm_feedback_data[row[\"Cluster\"]] = feedback\n",
        "        progress.append((i + 1) / len(cluster_representatives) * 100)\n",
        "        visualize_feedback_progress(progress)\n",
        "    return llm_feedback_data"
      ],
      "metadata": {
        "id": "gbzvQ3GppZLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def interpolate_llm_scores(trajectory_df_pca, llm_feedback_data):\n",
        "    \"\"\"\n",
        "    Interpolates LLM feedback scores across all timesteps.\n",
        "    \"\"\"\n",
        "    trajectory_df_pca[\"LLM_Adjusted_Score\"] = trajectory_df_pca[\"Cluster\"].map(llm_feedback_data)\n",
        "    trajectory_df_pca[\"LLM_Adjusted_Score\"] = trajectory_df_pca[\"LLM_Adjusted_Score\"].interpolate()\n",
        "    return trajectory_df_pca"
      ],
      "metadata": {
        "id": "tXMONDHxisQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_feedback_data = collect_llm_feedback_cluster(cluster_representatives)\n"
      ],
      "metadata": {
        "id": "WXyG474IetsF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trajectory_df_pca = interpolate_llm_scores(trajectory_df_pca, llm_feedback_data)"
      ],
      "metadata": {
        "id": "M37opHA7e0Vb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_table.enable_dataframe_formatter()\n",
        "data_table.DataTable(trajectory_df_pca)"
      ],
      "metadata": {
        "id": "Z6pCtzlxjBMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group the data by 'episode'\n",
        "episode_data = trajectory_df_pca.groupby('episode')\n",
        "# Loop through each episode\n",
        "for episode, data in episode_data:\n",
        "\n",
        "    cluster_list = data['Cluster'].tolist()\n",
        "\n",
        "    # Count the total number of time steps in the episode\n",
        "    total_timesteps = data['time_step'].max() + 1  # Assuming time_step starts from 0\n",
        "\n",
        "    print(f\"Episode {episode}:Total timesteps {total_timesteps}: {cluster_list}\")"
      ],
      "metadata": {
        "id": "fYtAL3RujBIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Section B.2: REWARD MODELLING(LLM FEEDBACK)\n",
        "\n",
        "*   Step B.2.1: Recalibrate Reward Based on LLM Feedback\n",
        "*   Step 3.2.2: Displaying the recalibrated rewards based on LLM feedback\n",
        "*   Step 3.2.3: Access the reward for a specific step"
      ],
      "metadata": {
        "id": "tSMYBxQNT2Nr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_llm_scores_to_dataframe(trajectory_df, trajectory_df_pca):\n",
        "    # Make a copy of the original dataframe\n",
        "    updated_df = trajectory_df.copy()\n",
        "\n",
        "    # Define regex patterns to extract scores\n",
        "    #score_1_pattern = r\"Action Effectiveness Evaluation: llm_score_1 = (\\d+)\"\n",
        "    #score_2_pattern = r\"Reward Appropriateness Evaluation: llm_score_2 = (\\d+)\"\n",
        "\n",
        "    score_1_pattern = r\"Action Effectiveness Evaluation: llm_score_1 = ([+-]?\\d+)\"\n",
        "    score_2_pattern = r\"Reward Appropriateness Evaluation: llm_score_2 = ([+-]?\\d+)\"\n",
        "\n",
        "    def extract_llm_scores(summary_text):\n",
        "        \"\"\"Extract llm_score_1 and llm_score_2 from the given text.\"\"\"\n",
        "        score_1_match = re.search(score_1_pattern, summary_text)\n",
        "        score_2_match = re.search(score_2_pattern, summary_text)\n",
        "\n",
        "        llm_score_1 = int(score_1_match.group(1)) if score_1_match else 0\n",
        "        llm_score_2 = int(score_2_match.group(1)) if score_2_match else 0\n",
        "\n",
        "        return llm_score_1, llm_score_2\n",
        "\n",
        "    # Extract LLM scores from trajectory_df_pca\n",
        "    llm_scores = trajectory_df_pca['LLM_Adjusted_Score'].apply(extract_llm_scores)\n",
        "\n",
        "    # Create new columns\n",
        "    updated_df['LLM_feedback_score'] = llm_scores.apply(lambda scores: f\"llm_score_1={scores[0]}, llm_score_2={scores[1]}\")\n",
        "    updated_df['LLM_score'] = llm_scores.apply(lambda scores: scores[0] + scores[1])\n",
        "\n",
        "    return updated_df"
      ],
      "metadata": {
        "id": "wkXYfTq6jw75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_feedback_df = add_llm_scores_to_dataframe(trajectory_df, trajectory_df_pca)"
      ],
      "metadata": {
        "id": "6Xe2mairjw4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_table.enable_dataframe_formatter()\n",
        "data_table.DataTable(llm_feedback_df)"
      ],
      "metadata": {
        "id": "BhZN8DGLzlmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group the data by 'episode'\n",
        "episode_data = llm_feedback_df.groupby('episode')\n",
        "# Loop through each episode\n",
        "for episode, data in episode_data:\n",
        "    # Extract lane indices\n",
        "    llm_score_list = data['LLM_score'].tolist()\n",
        "\n",
        "    # Count the total number of time steps in the episode\n",
        "    total_timesteps = data['time_step'].max() + 1  # Assuming time_step starts from 0\n",
        "\n",
        "    print(f\"Episode {episode}:Total timesteps {total_timesteps}: {llm_score_list}\")"
      ],
      "metadata": {
        "id": "iQlg0w3_jw1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step B.2.3: Recalibrate Reward Based on LLM Feedback\n",
        "# Function to recalibrate the rewards\n",
        "def recalibrate_rewards(df):\n",
        "    # Create a copy of the dataframe\n",
        "    df_copy = df.copy()\n",
        "\n",
        "    # Create the 'Recalibrated_rewards' column\n",
        "    df_copy['Recalibrated_rewards'] = df_copy['reward'] + df_copy['LLM_score']\n",
        "\n",
        "    # Get the list of recalibrated rewards\n",
        "    recalibrated_rewards_list = df_copy['Recalibrated_rewards'].tolist()\n",
        "\n",
        "    return df_copy, recalibrated_rewards_list"
      ],
      "metadata": {
        "id": "lBcjhCzAjwpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the function to recalibrate rewards\n",
        "recalibrated_df, recalibrated_rewards_list = recalibrate_rewards(llm_feedback_df)"
      ],
      "metadata": {
        "id": "VWEyihqymU-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_table.enable_dataframe_formatter()\n",
        "data_table.DataTable(recalibrated_df)"
      ],
      "metadata": {
        "id": "5puGyVstmU67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recalibrated_df.to_pickle('/content/drive/MyDrive/data_rp1/2_trajectories/2_llm_d/1_llm_feedback_ideal_df_with_sil_score.pkl')         # Update directory location 2"
      ],
      "metadata": {
        "id": "ak-TLG0Jmu22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To access the reward for a specific step:\n",
        "for i, feedback in enumerate(recalibrated_rewards_list):\n",
        "    human_recalibrated_reward_for_step = recalibrated_rewards_list[i]\n",
        "    print(f\"Recalibrated reward for step {i}: {human_recalibrated_reward_for_step}\")"
      ],
      "metadata": {
        "id": "dDuOe0VsmU5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SECTION A.5: MODEL TRAINING(HUMAN FEEDBACK DIRECT- IDEAL CASE SCENARIO)\n",
        "*   Step A.5.1: CUSTOM REWARD FUNCTION\n",
        "*   Step A.5.2: LOAD THE SAVED INITIALLY TRAINED PPO MODEL FROM GOOGLE DRIVE\n",
        "*   Step A.5.3: TRAIN/UPDATE PPO MODEL WITH RECALIBRATED REWARD\n",
        "*   Step A.5.4: SAVE THE TRAINED MODEL(HF_IDEAL) FOR TESTING"
      ],
      "metadata": {
        "id": "hFSLS54jm_yE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step A.5.1: CUSTOM REWARD FUNCTION\n",
        "def custom_reward(self, env, state, action, next_state, reward, done):\n",
        "    # Access and recalculate the reward using human_feedback_data or recalibrate_rewards_human function\n",
        "    global step_counter\n",
        "    try:\n",
        "        step_counter\n",
        "    except NameError:\n",
        "        step_counter = 0\n",
        "\n",
        "    reward = recalibrated_rewards_list[step_counter]\n",
        "    step_counter += 1\n",
        "    return reward\n",
        "\n",
        "# Create a new environment class that wraps your original environment and overrides the default reward function with your custom function\n",
        "class CustomRewardWrapper(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        super(CustomRewardWrapper, self).__init__(env)\n",
        "\n",
        "    def step(self, action):\n",
        "        next_state, reward, terminated, truncated, info = self.env.step(action)\n",
        "        done = terminated or truncated\n",
        "        reward = custom_reward(self, self.env, self.last_obs, action, next_state, reward, done)\n",
        "        # custom_reward should be defined and accessible to your class\n",
        "        self.last_obs = next_state\n",
        "        return next_state, reward, terminated, truncated, info\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        global step_counter\n",
        "        step_counter = 0\n",
        "        self.last_obs = self.env.reset(**kwargs)[0]  # Assuming Gymnasium env returns (obs, info)\n",
        "        return self.last_obs, {}  # Assuming Gymnasium env requires (obs, info)\n",
        "# Create and wrap the environment with your custom reward wrapper\n",
        "# env_human = CustomRewardWrapper(gym.make('highway-v0'))"
      ],
      "metadata": {
        "id": "AqogGzpsmU07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PPO training and Training logs"
      ],
      "metadata": {
        "id": "6-pv1yeKnGSo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive_log_dir = \"/content/drive/MyDrive/data_rp1/0_log_dir/7_ppo_highway_llmf_direct_ideal_1\"             # Update directory location 3"
      ],
      "metadata": {
        "id": "qeN5ejwRmUzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train PPO with Custom Rewards\n",
        "def train_ppo_with_custom_rewards(log_dir=drive_log_dir, total_timesteps=10000):\n",
        "    os.makedirs(log_dir, exist_ok=True)\n",
        "    env = CustomRewardWrapper(gym.make(\"highway-v0\"))\n",
        "    env = Monitor(env, log_dir)\n",
        "    model = PPO(\"MlpPolicy\", env, verbose=1, tensorboard_log=log_dir)\n",
        "    model.learn(total_timesteps=total_timesteps)\n",
        "    model.save('/content/drive/MyDrive/data_rp1/1_trained_models/7_ppo_highway_llmf_direct_ideal_1')      # Update directory location 4\n",
        "    return model, log_dir"
      ],
      "metadata": {
        "id": "jXmD4dl8mUw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# log_path = os.path.join(drive_log_dir, \"monitor.csv\")\n",
        "# df = pd.read_csv(log_path, skiprows=1)\n",
        "## Ensure episodes are logged correctly\n",
        "# df.reset_index(inplace=True)\n",
        "# df.rename(columns={\"index\": \"episode\", \"r\": \"reward\", \"l\": \"length\", \"t\": \"time_step\"}, inplace=True)"
      ],
      "metadata": {
        "id": "a5eOE4sfnP2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data_table.enable_dataframe_formatter()\n",
        "# data_table.DataTable(df)"
      ],
      "metadata": {
        "id": "yDjmFtMsnPyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Execute Training and Convergence Tracking\n",
        "model, log_dir = train_ppo_with_custom_rewards(total_timesteps=10000)"
      ],
      "metadata": {
        "id": "62lnvdO-nPs3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}