{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNjwDY5hbHLcFUGDxkevQ1d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RizanSM/zero_shot_llms_in_HIL_RL/blob/main/01_highway_env/02_default_env/06_BIASED_HF_RSM_CON/01_Policy_Training_BIASED_HF_RSM_CON.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HjMmE7sqibzf"
      },
      "outputs": [],
      "source": [
        "# Install the required libraries in your Google Colab environment\n",
        "!pip install stable-baselines3 gymnasium highway-env -q\n",
        "# Install necessary packages\n",
        "!pip install torch numpy pandas matplotlib -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import gymnasium as gym\n",
        "import highway_env\n",
        "import os"
      ],
      "metadata": {
        "id": "uOTBpEWwjEsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from google.colab import data_table\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "_bxF-amNjJgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Load and Preprocess the Data\n",
        "conservative_df =  pd.read_pickle('/content/drive/MyDrive/data_rp1/2_trajectories/1_human_feedback/3_Biased_Hf_D_Conservative_df.pkl')       # Update directory location 1"
      ],
      "metadata": {
        "id": "RL-lRgTvkPDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the data frame\n",
        "data_table.enable_dataframe_formatter()\n",
        "data_table.DataTable(conservative_df)"
      ],
      "metadata": {
        "id": "fGRR-WIPLpdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecting relevant features and target\n",
        "features = ['state', 'action', 'collision_flag', 'lane_index']\n",
        "target = 'Recalibrated_rewards'"
      ],
      "metadata": {
        "id": "fnkFTKa4ljg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert features into tensor-compatible format\n",
        "def process_features(df):\n",
        "    # X = np.stack(df['state'].apply(lambda x: np.array(x)).values)  # Convert state to numpy array\n",
        "    X = np.vstack(df['state'].apply(lambda x: np.array(x, dtype=np.float32)).values)  # Convert state to numpy array with float32 dtype\n",
        "    # X = np.hstack([X, df[['action', 'collision_flag', 'lane_index']].values])\n",
        "    X = np.hstack([X, df[['action', 'collision_flag', 'lane_index']].astype(np.float32).values])\n",
        "    y = df[target].values\n",
        "    return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "X, y = process_features(conservative_df)             # change here the dataframe name\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "id": "nqEdkRh1l9ll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into train and validation sets\n",
        "train_size = int(0.8 * len(X))\n",
        "val_size = len(X) - train_size\n",
        "train_data, val_data = random_split(TensorDataset(X, y), [train_size, val_size])\n",
        "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "38oYaiOtmBHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Define the Reward Model (Neural Network) with Dropout and L2 Regularization\n",
        "class RewardModel(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(RewardModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 64)\n",
        "        self.fc2 = nn.Linear(64, 32)\n",
        "        self.fc3 = nn.Linear(32, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.2)  # Added Dropout\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.dropout(x)  # Apply dropout after activation\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "        return self.fc3(x)\n",
        "\n",
        "# Initialize model\n",
        "input_dim = X.shape[1]\n",
        "print(input_dim)\n",
        "model = RewardModel(input_dim)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.SmoothL1Loss()  # Replaced MSE with Huber Loss\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)  # Switched to AdamW and added L2 Regularization\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)  # Added ReduceLROnPlateau\n",
        "\n",
        "# Early stopping parameters\n",
        "patience = 10  # Increased patience for early stopping\n",
        "min_delta = 0.0001\n",
        "best_loss = float('inf')\n",
        "patience_counter = 0\n",
        "train_losses = []\n",
        "val_losses = []"
      ],
      "metadata": {
        "id": "4oHkWLN2mF4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Train the Reward Model with Early Stopping\n",
        "num_epochs = 50\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch_X, batch_y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_X).squeeze()\n",
        "        loss = criterion(outputs, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    train_loss = total_loss / len(train_loader)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    # Validation loss\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_loss = sum(criterion(model(batch_X).squeeze(), batch_y).item() for batch_X, batch_y in val_loader) / len(val_loader)\n",
        "\n",
        "    val_losses.append(val_loss)\n",
        "    scheduler.step(val_loss)  # Apply learning rate scheduler\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    # Early stopping criteria\n",
        "    if val_loss < best_loss - min_delta:\n",
        "        best_loss = val_loss\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "\n",
        "    if patience_counter >= patience:\n",
        "        print(\"Early stopping triggered!\")\n",
        "        break\n",
        "\n",
        "# Plot losses\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Training & Validation Losses')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jrv4MsQpmXOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Train PPO Using the Trained Reward Model\n",
        "\n",
        "# Custom reward function using the trained reward model\n",
        "def custom_reward(env, state):\n",
        "    state_tensor = torch.tensor(np.array(state), dtype=torch.float32).unsqueeze(0)\n",
        "    return model(state_tensor).item()\n",
        "\n",
        "# Custom environment wrapper to replace environment reward with learned reward\n",
        "class CustomHighwayEnv(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        super(CustomHighwayEnv, self).__init__(env)\n",
        "\n",
        "    def step(self, action):\n",
        "        next_state, _, done, truncated, info = self.env.step(action)\n",
        "        reward = custom_reward(self.env, next_state)  # Use learned reward\n",
        "        return next_state, reward, done, truncated, info\n"
      ],
      "metadata": {
        "id": "15gWC2k3mL8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Train PPO Using the Trained Reward Model\n",
        "\n",
        "# Custom reward function using the trained reward model\n",
        "def custom_reward(env, state):\n",
        "    # Debugging: Print the state shape before processing\n",
        "    # print(\"State shape before processing:\", state.shape)\n",
        "\n",
        "    # Reshape the state to match the expected input dimension of the RewardModel\n",
        "    state_tensor = torch.tensor(state, dtype=torch.float32).reshape(1, -1)  # Reshape to (1, num_features)\n",
        "\n",
        "\n",
        "    # Ensure the state tensor has the correct dimensions for the model\n",
        "    input_dim = model.fc1.in_features  # Get expected input dimension from model\n",
        "    if state_tensor.shape[1] < input_dim:\n",
        "        padding = torch.zeros((1, input_dim - state_tensor.shape[1]))\n",
        "        state_tensor = torch.cat([state_tensor, padding], dim=1)\n",
        "\n",
        "    # Reshape to (1, num_features), and slice off any extra dimensions that are not expected.\n",
        "    # state_tensor = torch.tensor(state, dtype=torch.float32).reshape(1, -1)[:, :input_dim]\n",
        "    return model(state_tensor).item()\n",
        "\n",
        "# Custom environment wrapper to replace environment reward with learned reward\n",
        "class CustomHighwayEnv(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        super(CustomHighwayEnv, self).__init__(env)\n",
        "\n",
        "    def step(self, action):\n",
        "        next_state, _, done, truncated, info = self.env.step(action)\n",
        "        reward = custom_reward(self.env, next_state)  # Use learned reward\n",
        "        return next_state, reward, done, truncated, info"
      ],
      "metadata": {
        "id": "2zvBweZXmh6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PPO training and Training logs"
      ],
      "metadata": {
        "id": "vfCtoN8IDByf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# env = CustomHighwayEnv(gym.make('highway-v0'))\n",
        "# env = DummyVecEnv([lambda: env])"
      ],
      "metadata": {
        "id": "Sj_9G3Xo1qoh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive_log_dir = \"/content/drive/MyDrive/data_rp1/0_log_dir/6_ppo_highway_biased_hf_lrs_conservative\"              # Update directory location 2"
      ],
      "metadata": {
        "id": "eVebcubSCPZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train PPO with Custom Rewards\n",
        "def train_ppo_with_custom_rewards(log_dir=drive_log_dir, total_timesteps=10000):\n",
        "    os.makedirs(log_dir, exist_ok=True)\n",
        "    env = CustomHighwayEnv(gym.make(\"highway-v0\"))\n",
        "    env = Monitor(env, log_dir)\n",
        "    model = PPO(\"MlpPolicy\", env, verbose=1, tensorboard_log=log_dir)\n",
        "    model.learn(total_timesteps=total_timesteps)\n",
        "    model.save('/content/drive/MyDrive/data_rp1/1_trained_models/6_ppo_highway_biased_hf_lrs_conservative')         # Update directory location 3\n",
        "    return model, log_dir"
      ],
      "metadata": {
        "id": "wgoucJqUCT6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# log_path = os.path.join(drive_log_dir, \"monitor.csv\")\n",
        "# df = pd.read_csv(log_path, skiprows=1)\n",
        "## Ensure episodes are logged correctly\n",
        "# df.reset_index(inplace=True)\n",
        "# df.rename(columns={\"index\": \"episode\", \"r\": \"reward\", \"l\": \"length\", \"t\": \"time_step\"}, inplace=True)"
      ],
      "metadata": {
        "id": "jCY0zH1SCVkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data_table.enable_dataframe_formatter()\n",
        "# data_table.DataTable(df)"
      ],
      "metadata": {
        "id": "Ww_sehOjCVet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Execute Training and Convergence Tracking\n",
        "model, log_dir = train_ppo_with_custom_rewards(total_timesteps=10000)"
      ],
      "metadata": {
        "id": "Q4z70eRPCVai"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}